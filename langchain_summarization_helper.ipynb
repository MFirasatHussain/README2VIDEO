{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hny4I-ODTIS6"
   },
   "source": [
    "# Text Summarization of Large Documents using LangChain 🦜🔗\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nLS57E2TO5y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Text summarization is an NLP task that creates a concise and informative summary of a longer text. LLMs can be used to create summaries of news articles, research papers, technical documents, and other types of text.\n",
    "\n",
    "Summarizing large documents can be challenging. To create summaries, you need to apply summarization strategies to your indexed documents. You have already seen some of these strategies in the previous notebooks. If you haven't completed it, it is recommended to do so to have a basic understanding of how to summarize large documents.\n",
    "\n",
    "In this notebook, you will use LangChain, a framework for developing LLM applications, to apply some summarization strategies. The notebook covers several examples of how to summarize large documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXsvgIuwTPZw"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use LangChain with PaLM API to summarize large documents by working through the following examples:\n",
    "\n",
    "- Stuffing method\n",
    "- MapReduce method\n",
    "- Refine method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skXAu__iqks_"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI Generative AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvKl-BtQTRiQ"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwFMpIMrTV_4"
   },
   "source": [
    "### Install Vertex AI SDK & Other dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5fXfvzhTkYN"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install openai tiktoken chromadb langchain"
   ],
   "metadata": {
    "id": "P08aIGuhuJz9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4300dc14-6b00-4f66-e884-4bbd75c358ce"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.3.3-py3-none-any.whl (220 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m220.3/220.3 kB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m33.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting chromadb\n",
      "  Downloading chromadb-0.4.17-py3-none-any.whl (496 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m496.8/496.8 kB\u001B[0m \u001B[31m37.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting langchain\n",
      "  Downloading langchain-0.0.338-py3-none-any.whl (2.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m57.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.0/75.0 kB\u001B[0m \u001B[31m8.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m42.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m92.9/92.9 kB\u001B[0m \u001B[31m11.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.7/59.7 kB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
      "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.4/5.4 MB\u001B[0m \u001B[31m66.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.4/6.4 MB\u001B[0m \u001B[31m51.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.9/57.9 kB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m105.3/105.3 kB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.3/67.3 kB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.2)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m593.7/593.7 kB\u001B[0m \u001B[31m46.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m78.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
      "  Downloading langsmith-0.0.65-py3-none-any.whl (46 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.1/46.1 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.4/49.4 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.0/67.0 kB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting typing-extensions<5,>=4.5 (from openai)\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.9/76.9 kB\u001B[0m \u001B[31m8.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m143.8/143.8 kB\u001B[0m \u001B[31m18.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
      "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.8/50.8 kB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m341.4/341.4 kB\u001B[0m \u001B[31m25.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m71.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m60.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m130.2/130.2 kB\u001B[0m \u001B[31m15.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m10.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=51eb39b8f23a93dc14b3d35fc938fa826b8169510e60eb78ed076250dfba20b8\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, websockets, uvloop, urllib3, typing-extensions, python-dotenv, pulsar-client, overrides, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typing-inspect, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, httpcore, coloredlogs, tiktoken, posthog, opentelemetry-sdk, onnxruntime, langsmith, httpx, fastapi, dataclasses-json, opentelemetry-exporter-otlp-proto-grpc, openai, langchain, kubernetes, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lida 0.0.10 requires kaleido, which is not installed.\n",
      "lida 0.0.10 requires python-multipart, which is not installed.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\n",
      "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.17 coloredlogs-15.0.1 dataclasses-json-0.6.2 deprecated-1.2.14 fastapi-0.104.1 h11-0.14.0 httpcore-1.0.2 httptools-0.6.1 httpx-0.25.1 humanfriendly-10.0 jsonpatch-1.33 jsonpointer-2.4 kubernetes-28.1.0 langchain-0.0.338 langsmith-0.0.65 marshmallow-3.20.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.16.2 openai-1.3.3 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tiktoken-0.5.1 typing-extensions-4.8.0 typing-inspect-0.9.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install openai==0.28.1"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "HPkR_H0Gzfi2",
    "outputId": "d0c403fb-ae80-4926-bbaa-34f5b930c3bb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting openai==0.28.1\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/77.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m71.7/77.0 kB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.0/77.0 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed openai-0.28.1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "openai"
        ]
       }
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip show openai"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huU1HSr1vegG",
    "outputId": "81275ebd-7192-4cf1-8549-82d33a247039"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Name: openai\n",
      "Version: 1.3.3\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: anyio, distro, httpx, pydantic, tqdm, typing-extensions\n",
      "Required-by: llmx\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRkcfnQMT9vD"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.llms import VertexAI\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAGaTjPVTmhP"
   },
   "source": [
    "### Import models\n",
    "\n",
    "You load the pre-trained text generation model called `text-bison@001`."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"falcon_1ft_20231127_134856-z4pob5q6\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, torch_dtype=torch.float16, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        # return prompt[: self.n]\n",
    "        prompt_template=f'''{prompt}'''\n",
    "        input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "        output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=1024)\n",
    "        return tokenizer.decode(output[0])\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"param\": 2}"
   ],
   "metadata": {
    "id": "LRKIdRwykqYo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITUmZiNZcMUW"
   },
   "outputs": [],
   "source": [
    "llm_gpt_turbo = CustomLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKG-ZTJ_02wq"
   },
   "source": [
    "## Summarization with Large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZkLDRTjTcfm"
   },
   "source": [
    "### Preparing data files\n",
    "\n",
    "To begin, you will need to download a few files that are required for the summarizing tasks below."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import markdown\n",
    "import re\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Function to extract text from a Markdown file\n",
    "def extract_text_from_readme(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        markdown_content = file.read()\n",
    "\n",
    "        # Parse the Markdown content\n",
    "        html_content = markdown.markdown(markdown_content)\n",
    "\n",
    "        # Remove HTML tags to get plain text\n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "\n",
    "        return text\n",
    "\n",
    "# Specify the path to your README.md file\n",
    "# readme_path = 'drive/MyDrive/README.md'\n",
    "readme_path = 'README_sizelim.md'\n",
    "\n",
    "# Extract text from the README.md file\n",
    "readme_text = extract_text_from_readme(readme_path)\n",
    "\n",
    "# Print the extracted text\n",
    "# print(readme_text.strip(\" \").replace(\"\\n\",\"\"))\n",
    "re.sub(r'[\\r\\n][\\r\\n]{2,}', '\\n\\n', readme_text)\n",
    "readme_doc = Document(\n",
    "        page_content=readme_text,\n",
    "        metadata={\"source\": \"\"},\n",
    "    )"
   ],
   "metadata": {
    "id": "F5pt3zRe4P2y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2d1H40WtZ_z",
    "outputId": "d0e7bafe-6ff9-4e44-f920-6b7be3cbfdb2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDVwBFSjZ7ws"
   },
   "source": [
    "## Method 1: Stuffing\n",
    "\n",
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want.\n",
    "\n",
    "In LangChain, you can use `StuffDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is setting `stuff` as `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhEi-XqKnv2v"
   },
   "source": [
    "### Prompt design with `Stuffing` chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-ljajUen1YO"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5aVrDWkJs3Y"
   },
   "source": [
    "### Retrying\n",
    "Initiate a chain using `stuff` method and process three pages document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_hoizIgObe9"
   },
   "outputs": [],
   "source": [
    "stuff_chain = load_summarize_chain(llm_gpt_turbo, chain_type=\"stuff\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "stuff_chain.run([readme_doc])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "CBWZxgBG-66n",
    "outputId": "33b53620-ff08-49bb-dfb8-2c853fa1b3c2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'- LangChain is a library for building applications with Large Language Models (LLMs) through composability.\\n- LangSmith is a developer platform for building, testing, and monitoring LLM applications.\\n- Select chains (SQLDatabase) will be moved to langchain_experimental for a leaner and safer LangChain.\\n- Quick installation can be done with pip install langchain or pip install langsmith && conda install langchain -c conda-forge.\\n- LangChain aims to assist in the development of applications such as question answering, chatbots, and agents.\\n- LangChain provides documentation, examples, and resources for getting started and using the library.\\n- LangChain helps with prompt management, chains, data augmented generation, agents, memory, and evaluation.\\n- Contributions to the project are welcome.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 16
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnXUwWxkrLu4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a5a2beb8-d85a-474b-c2a5-ab27d56a19eb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The code failed since it won't be able to run inference on such a huge context and throws this exception:  'tuple' object has no attribute 'page_content'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(stuff_chain.run(readme_doc))\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
    "        e,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqZrKM32h-o2"
   },
   "source": [
    "### Considerations\n",
    "\n",
    "The `stuffing` method is a way to summarize text by feeding the entire document to a large language model (LLM) in a single call. This method has both pros and cons.\n",
    "\n",
    "The stuffing method only requires a single call to the LLM, which can be faster than other methods that require multiple calls. When summarizing text, the LLM has access to all the data at once, which can result in a better summary.\n",
    "\n",
    "But, LLMs have a context length, which is the maximum number of tokens that can be processed in a single call. If the document is longer than the context length, the stuffing method will not work. Also the stuffing method is not suitable for summarizing large documents, as it can be slow and may not produce a good summary.\n",
    "\n",
    "Let's explore other approaches to help deal with having longer text than context lengh limit of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM3V1JARZ9-k"
   },
   "source": [
    "## Method 2: MapReduce\n",
    "\n",
    "The `MapReduce` method implements a multi-stage summarization. It is a technique for summarizing large pieces of text by first summarizing smaller chunks of text and then combining those summaries into a single summary.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is setting `map_reduce` as `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lagLXEamlPY2"
   },
   "source": [
    "### Prompt design with `MapReduce` chain\n",
    "\n",
    "In our example, you have a 32-page document that you need to summarize.\n",
    "\n",
    "With LangChain, the `map_reduce` chain breaks the document down into 1024 token chunks max. Then it runs the initial prompt you define on each chunk to generate a summary of that chunk. In the example below, you use the following first stage or map prompt.\n",
    "\n",
    "```Write a concise summary of the following text delimited by triple backquotes. Return your response in bullet points which covers the key points of the text.\n",
    "'''{text}'''. BULLET POINT SUMMARY:```\n",
    "\n",
    "Once summaries for all of the chunks are generated, it runs a different prompt to combine those summaries into a single summary. In the example below, you use the following second stage or combine prompt.\n",
    "\n",
    "```Write a summary of the entire document that includes the main points from all of the individual summaries.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6oHEtdSmsTn"
   },
   "outputs": [],
   "source": [
    "map_prompt_template = \"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# combine_prompt_template = \"\"\"\n",
    "#                       Write a  summary of around 400 words for the following text delimited by triple backquotes.\n",
    "#                       Return your response in bullet points which covers the key points of the text.\n",
    "#                       ```{text}```\n",
    "#                       BULLET POINT SUMMARY:\n",
    "#                       \"\"\"\n",
    "combine_prompt_template = \"\"\"\n",
    "                      Write a  summary of around 400 words for the following text delimited by triple backquotes.\n",
    "                      ```{text}```\n",
    "                      SUMMARY:\n",
    "                      \"\"\"\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXoz0uLDMoWD"
   },
   "source": [
    "### Generate summaries using MapReduce method\n",
    "\n",
    "After defining prompts, you initialize the associated `map_reduce_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRGJcBZeVdEa"
   },
   "outputs": [],
   "source": [
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm_gpt_turbo,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6fekDDr0hrJ"
   },
   "source": [
    "Then, you generate summaries using the chain. Notice that LangChain use a tokenizer (from transformer library) with 1024 token limit by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSC6w2TBV35q"
   },
   "outputs": [],
   "source": [
    "map_reduce_outputs = map_reduce_chain({\"input_documents\": [readme_doc]})"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "map_reduce_outputs"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgX9VAFIwZd4",
    "outputId": "f3c1d306-6798-43a2-ed4e-b5a52d4d17a0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Size Limit \\n\\nSize Limit is a performance budget tool for JavaScript. It checks every commit\\non CI, calculates\\xa0the real cost of\\xa0your JS for end-users and throws an error\\nif the cost exceeds the\\xa0limit.\\n\\nES modules and tree-shaking support.\\nAdd Size Limit to GitHub Actions, Circle CI or another CI system\\n  to know if a pull request adds a\\xa0massive\\xa0dependency.\\nModular to fit different use cases: big JS applications\\n  that use their own bundler or\\xa0small\\xa0npm\\xa0libraries\\xa0with\\xa0many files.\\nCan calculate the time it would take a browser\\n  to download and execute your JS. Time\\xa0is\\xa0a\\xa0much\\xa0more\\xa0accurate\\n  and\\xa0understandable metric compared to the size in bytes.\\nCalculations include all dependencies and polyfills\\n  used in your JS.\\n\\n\\n\\n\\nWith GitHub action Size Limit will post bundle size changes as a comment\\nin pull request discussion.\\n\\n\\n\\nWith --why, Size Limit can tell you why your library is of this size\\nand show the real cost of all your internal dependencies.\\nWe are using Statoscope for this analysis.\\n\\n\\n\\n\\n\\n\\n\\n\\nWho Uses Size Limit\\n\\nMobX\\nMaterial-UI\\nAutoprefixer\\nPostCSS reduced\\n  25% of the size.\\nBrowserslist reduced\\n  25% of the size.\\nEmojiMart reduced\\n  20% of the size\\nnanoid reduced\\n  33% of the size.\\nReact Focus Lock reduced\\n  32% of the size.\\nLogux reduced\\n  90% of the size.\\n\\nHow It Works\\n\\nSize Limit contains a CLI tool, 3 plugins (file, webpack, time)\\n   and 3 plugin presets for popular use cases (app, big-lib, small-lib).\\n   A CLI tool finds plugins in package.json and loads the config.\\nIf you use the webpack plugin, Size Limit will bundle your JS files into\\n   a single file. It is important to track dependencies and\\xa0webpack polyfills.\\n   It is also useful for small libraries with many small files and without\\n   a bundler.\\nThe webpack plugin creates an empty webpack project, adds your library\\n   and looks for the bundle size difference.\\nThe time plugin compares the current machine performance with that of\\n   a low-priced Android devices to calculate the CPU throttling rate.\\nThen the time plugin runs headless Chrome (or desktop Chrome if it’s\\n   available) to\\xa0track the time a browser takes to\\xa0compile and execute your JS.\\n   Note that these measurements depend on available resources and might\\n   be unstable. See here\\n   for more details.\\n\\nUsage\\nJS Applications\\nSuitable for applications that have their own bundler and send the JS bundle\\ndirectly to a client (without publishing it to npm). Think of a user-facing app\\nor website, like an email client, a CRM, a landing page or a blog with\\ninteractive elements, using React/Vue/Svelte lib or vanilla JS.\\nShow instructions\\n\\n1. Install the preset:\\n\\n    ```sh\\n    npm install --save-dev size-limit @size-limit/file\\n    ```\\n\\n2. Add the `size-limit` section and the `size` script to your `package.json`:\\n\\n    ```diff\\n    + \"size-limit\": [\\n    +   {\\n    +     \"path\": \"dist/app-*.js\"\\n    +   }\\n    + ],\\n      \"scripts\": {\\n        \"build\": \"webpack ./webpack.config.js\",\\n    +   \"size\": \"npm run build && size-limit\",\\n        \"test\": \"vitest && eslint .\"\\n      }\\n    ```\\n\\n3. Here’s how you can get the size for your current project:\\n\\n    ```sh\\n    $ npm run size\\n\\n      Package size: 30.08 kB with all dependencies, minified and brotlied\\n    ```\\n\\n4. Now, let’s set the limit. Add 25% to the current total size and use that as\\n   the limit in your `package.json`:\\n\\n    ```diff\\n      \"size-limit\": [\\n        {\\n    +     \"limit\": \"35 kB\",\\n          \"path\": \"dist/app-*.js\"\\n        }\\n      ],\\n    ```\\n\\n5. Add the `size` script to your test suite:\\n\\n    ```diff\\n      \"scripts\": {\\n        \"build\": \"webpack ./webpack.config.js\",\\n        \"size\": \"npm run build && size-limit\",\\n    -   \"test\": \"vitest && eslint .\"\\n    +   \"test\": \"vitest && eslint . && npm run size\"\\n      }\\n    ```\\n\\n6. If you don’t have a continuous integration service running, don’t forget\\n   to add one —\\xa0start with Github Actions.\\n\\n\\nJS Application and Time-based Limit\\nFile size limit (in kB) is not the best way to describe your JS application\\ncost for developers. Developers will compare the size of the JS bundle\\nwith the size of images. But browsers need much more time to parse 100 kB\\nof JS than 100 kB of an image since JS compilers are very complex.\\nThis is why Size Limit support time-based limit. It runs headless Chrome\\nto track the time a browser takes to compile and execute your JS.\\nShow instructions\\n\\n1. Install the preset:\\n\\n    ```sh\\n    npm install --save-dev size-limit @size-limit/preset-app\\n    ```\\n\\n2. Add the `size-limit` section and the `size` script to your `package.json`:\\n\\n    ```diff\\n    + \"size-limit\": [\\n    +   {\\n    +     \"path\": \"dist/app-*.js\"\\n    +   }\\n    + ],\\n      \"scripts\": {\\n        \"build\": \"webpack ./webpack.config.js\",\\n    +   \"size\": \"npm run build && size-limit\",\\n        \"test\": \"vitest && eslint .\"\\n      }\\n    ```\\n\\n3. Here’s how you can get the size for your current project:\\n\\n    ```sh\\n    $ npm run size\\n\\n      Package size: 30.08 kB with all dependencies, minified and brotlied\\n      Loading time: 602 ms   on slow 3G\\n      Running time: 214 ms   on Snapdragon 410\\n      Total time:   815 ms\\n    ```\\n\\n4. Now, let’s set the limit. Add 25% to the current total time and use that as\\n   the limit in your `package.json`:\\n\\n    ```diff\\n      \"size-limit\": [\\n        {\\n    +     \"limit\": \"1 s\",\\n          \"path\": \"dist/app-*.js\"\\n        }\\n      ],\\n    ```\\n\\n5. Add the `size` script to your test suite:\\n\\n    ```diff\\n      \"scripts\": {\\n        \"build\": \"webpack ./webpack.config.js\",\\n        \"size\": \"npm run build && size-limit\",\\n    -   \"test\": \"vitest && eslint .\"\\n    +   \"test\": \"vitest && eslint . && npm run size\"\\n      }\\n    ```\\n\\n6. If you don’t have a continuous integration service running, don’t forget\\n   to add one —\\xa0start with Github Actions.\\n\\n\\nBig Libraries\\nJS libraries > 10 kB in size.\\nThis preset includes headless Chrome, and will measure your lib’s execution\\ntime. You likely don’t need this overhead for a small 2 kB lib, but for larger\\nones the execution time is a more accurate and understandable metric that\\nthe size in bytes. Libraries like React are good examples for this preset.\\nShow instructions\\n\\n1. Install preset:\\n\\n    ```sh\\n    npm install --save-dev size-limit @size-limit/preset-big-lib\\n    ```\\n\\n2. Add the `size-limit` section and the `size` script to your `package.json`:\\n\\n    ```diff\\n    + \"size-limit\": [\\n    +   {\\n    +     \"path\": \"dist/react.production-*.js\"\\n    +   }\\n    + ],\\n      \"scripts\": {\\n        \"build\": \"webpack ./scripts/rollup/build.js\",\\n    +   \"size\": \"npm run build && size-limit\",\\n        \"test\": \"vitest && eslint .\"\\n      }\\n    ```\\n\\n3. If you use ES modules you can test the size after tree-shaking with `import`\\n   option:\\n\\n    ```diff\\n      \"size-limit\": [\\n        {\\n          \"path\": \"dist/react.production-*.js\",\\n    +     \"import\": \"{ createComponent }\"\\n        }\\n      ],\\n    ```\\n\\n4. Here’s how you can get the size for your current project:\\n\\n    ```sh\\n    $ npm run size\\n\\n      Package size: 30.08 kB with all dependencies, minified and brotlied\\n      Loading time: 602 ms   on slow 3G\\n      Running time: 214 ms   on Snapdragon 410\\n      Total time:   815 ms\\n    ```\\n\\n5. Now, let’s set the limit. Add 25% to the current total time and use that\\n   as the limit in your `package.json`:\\n\\n    ```diff\\n      \"size-limit\": [\\n        {\\n    +     \"limit\": \"1 s\",\\n          \"path\": \"dist/react.production-*.js\"\\n        }\\n      ],\\n    ```\\n\\n6. Add a `size` script to your test suite:\\n\\n    ```diff\\n      \"scripts\": {\\n        \"build\": \"rollup ./scripts/rollup/build.js\",\\n        \"size\": \"npm run build && size-limit\",\\n    -   \"test\": \"vitest && eslint .\"\\n    +   \"test\": \"vitest && eslint . && npm run size\"\\n      }\\n    ```\\n\\n7. If you don’t have a continuous integration service running, don’t forget\\n   to add one —\\xa0start with Github Actions.\\n8. Add the library size to docs, it will help users to choose your project:\\n\\n    ```diff\\n      # Project Name\\n\\n      Short project description\\n\\n      * **Fast.** 10% faster than competitor.\\n    + * **Small.** 15 kB (minified and brotlied).\\n    +   [Size Limit](https://github.com/ai/size-limit) controls the size.\\n    ```\\n\\n\\nSmall Libraries\\nJS libraries < 10 kB in size.\\nThis preset will only measure the size, without the execution time, so it’s\\nsuitable for small libraries. If your library is larger, you likely want\\nthe Big Libraries preset above. [Nano\\xa0ID]\\xa0or\\xa0Storeon\\xa0are\\xa0good\\xa0examples\\nfor this preset.\\nShow instructions\\n\\n1. First, install `size-limit`:\\n\\n    ```sh\\n    npm install --save-dev size-limit @size-limit/preset-small-lib\\n    ```\\n\\n2. Add the `size-limit` section and the `size` script to your `package.json`:\\n\\n    ```diff\\n    + \"size-limit\": [\\n    +   {\\n    +     \"path\": \"index.js\"\\n    +   }\\n    + ],\\n      \"scripts\": {\\n    +   \"size\": \"size-limit\",\\n        \"test\": \"vitest && eslint .\"\\n      }\\n    ```\\n\\n3. Here’s how you can get the size for your current project:\\n\\n    ```sh\\n    $ npm run size\\n\\n      Package size: 177 B with all dependencies, minified and brotlied\\n    ```\\n\\n4. If your project size starts to look bloated, run `--why` for analysis:\\n\\n    ```sh\\n    npm run size -- --why\\n    ```\\n\\n    > We use [Statoscope](https://github.com/statoscope/statoscope) as bundle analyzer.\\n\\n5. Now, let’s set the limit. Determine the current size of your library,\\n   add just a little bit (a kilobyte, maybe) and\\xa0use\\xa0that\\xa0as\\xa0the\\xa0limit\\n   in your `package.json`:\\n\\n    ```diff\\n     \"size-limit\": [\\n        {\\n    +     \"limit\": \"9 kB\",\\n          \"path\": \"index.js\"\\n        }\\n     ],\\n    ```\\n\\n6. Add the `size` script to your test suite:\\n\\n    ```diff\\n      \"scripts\": {\\n        \"size\": \"size-limit\",\\n    -   \"test\": \"vitest && eslint .\"\\n    +   \"test\": \"vitest && eslint . && npm run size\"\\n      }\\n    ```\\n\\n7. If you don’t have a continuous integration service running, don’t forget\\n   to add one —\\xa0start with Github Actions.\\n8. Add the library size to docs, it will help users to choose your project:\\n\\n    ```diff\\n      # Project Name\\n\\n      Short project description\\n\\n      * **Fast.** 10% faster than competitor.\\n    + * **Small.** 500 bytes (minified and brotlied). No\\xa0dependencies.\\n    +   [Size Limit](https://github.com/ai/size-limit) controls the size.\\n    ```\\n\\n\\nReports\\nSize Limit has a GitHub action that comments and rejects pull requests based\\non Size Limit output.\\n\\nInstall and configure Size Limit as shown above.\\nAdd the following action inside .github/workflows/size-limit.yml\\n\\nyaml\\nname: \"size\"\\non:\\n  pull_request:\\n    branches:\\n      - master\\njobs:\\n  size:\\n    runs-on: ubuntu-latest\\n    env:\\n      CI_JOB_NUMBER: 1\\n    steps:\\n      - uses: actions/checkout@v1\\n      - uses: andresz1/size-limit-action@v1\\n        with:\\n          github_token: ${{ secrets.GITHUB_TOKEN }}\\nConfig\\nPlugins and Presets\\nPlugins or plugin presets will be loaded automatically from package.json.\\nFor example, if you want to use @size-limit/webpack, you can just use\\nnpm install --save-dev @size-limit/webpack, or you can use our preset\\n@size-limit/preset-big-lib.\\nPlugins:\\n\\n@size-limit/file checks the size of files with Brotli (default), Gzip\\n  or without compression.\\n@size-limit/webpack adds your library to empty webpack project\\n  and prepares bundle file for file plugin.\\n@size-limit/webpack-why adds reports for webpack plugin\\n  about your library is of this size to show the cost of all your\\n  dependencies.\\n@size-limit/webpack-css adds css support for webpack plugin.\\n@size-limit/esbuild is like webpack plugin, but uses esbuild\\n  to be faster and use less space in node_modules.\\n@size-limit/esbuild-why add reports for esbuild plugin\\n  about your library is of this size to show the cost of all your\\n  dependencies.\\n@size-limit/time uses headless Chrome to track time to execute JS.\\n\\nPlugin presets:\\n\\n@size-limit/preset-app contains file and time plugins.\\n@size-limit/preset-big-lib contains webpack, file, and time plugins.\\n@size-limit/preset-small-lib contains esbuild and file plugins.\\n\\nThird-Party Plugins\\nThird-party plugins and presets named starting with size-limit- are also supported.\\nFor example:\\n\\nsize-limit-node-esbuild\\n  is like @size-limit/esbuild but for Node libraries.\\nsize-limit-preset-node-lib\\n  is like @size-limit/preset-small-lib but for Node libraries which contains\\n  above node-esbuild and core file plugins.\\nnx-size-limit\\n  is an NX build system community plugin.\\n\\nLimits Config\\nSize Limits supports three ways to define limits config.\\n\\nsize-limit section in package.json:\\n\\njson\\n     \"size-limit\": [\\n       {\\n         \"path\": \"index.js\",\\n         \"import\": \"{ createStore }\",\\n         \"limit\": \"500 ms\"\\n       }\\n     ]\\n\\nor a separate .size-limit.json config file:\\n\\njs\\n   [\\n     {\\n       \"path\": \"index.js\",\\n       \"import\": \"{ createStore }\",\\n       \"limit\": \"500 ms\"\\n     }\\n   ]\\n\\nor a more flexible .size-limit.js or .size-limit.cjs config file:\\n\\njs\\n   module.exports = [\\n     {\\n       path: \"index.js\",\\n       import: \"{ createStore }\",\\n       limit: \"500 ms\"\\n     }\\n   ]\\nEach section in the config can have these options:\\n\\npath: relative paths to files. The only mandatory option.\\n  It could be a path \"index.js\", a pattern \"dist/app-*.js\"\\n  or\\xa0an\\xa0array [\"index.js\", \"dist/app-*.js\", \"!dist/app-exclude.js\"].\\nimport: partial import to test tree-shaking. It could be \"{ lib }\"\\n  to test import { lib } from \\'lib\\', * to test all exports,\\n  or { \"a.js\": \"{ a }\", \"b.js\": \"{ b }\" } to test multiple files.\\nlimit: size or time limit for files from the path option. It should be\\n  a string with a number and unit, separated by a space.\\n  Format: 100 B, 10 kB, 500 ms, 1 s.\\nname: the name of the current section. It will only be useful\\n  if you have multiple sections.\\nentry: when using a custom webpack config, a webpack entry could be given.\\n  It could be a string or an array of strings.\\n  By default, the total size of all entry points will be checked.\\nwebpack: with false it will disable webpack.\\nrunning: with false it will disable calculating running time.\\ngzip: with true it will use Brotli compression and disable\\n  Brotli compression.\\nbrotli: with false it will disable any compression.\\nconfig: a path to a custom webpack config.\\nignore: an array of files and dependencies to exclude from\\n  the project size calculation.\\nmodifyWebpackConfig: (.size-limit.js only) function that can be used\\n  to do last-minute changes to the webpack config, like adding a plugin.\\ncompareWith: path to stats.json from another build to compare\\n  (when --why is using).\\nuiReports: custom UI reports list (see Statoscope docs).\\n\\nIf you use Size Limit to track the size of CSS files, make sure to set\\nwebpack: false. Otherwise, you will get wrong numbers, because webpack\\ninserts style-loader runtime (≈2 kB) into the bundle.\\nAnalyze with --why\\nYou can run size-limit --why to analyze the bundle.\\nYou will need to install @size-limit/esbuild-why or @size-limit/webpack-why\\ndepends on which bundler you are using (default is esbuild).\\nFor @size-limit/esbuild-why,\\nit will generate a esbuild-why.html at the current directory & open it in the browser.\\nIf you also specify --save-bundle <DIR>,\\nthe report will be generated inside <DIR>.\\nIf you have multiple sections in your config,\\nthe files will be named esbuild-why-{n}.html,\\nor you can give it a custom name:\\njsonc\\n[\\n  {\\n    \"name\": \"cjs\",\\n    /* snap */\\n  },\\n  {\\n    \"name\": \"esm\",\\n    /* snap */\\n  }\\n]\\nThis will produce esbuild-why-cjs.html and esbuild-why-esm.html respectively.\\nFor @size-limit/webpack-why,\\nit will generate the report and open it in the browser automatically.\\nJS API\\n```js\\nconst sizeLimit = require(\\'size-limit\\')\\nconst filePlugin = require(\\'@size-limit/file\\')\\nconst webpackPlugin = require(\\'@size-limit/webpack\\')\\nsizeLimit([filePlugin, webpackPlugin], [filePath]).then(result => {\\n  result //=> { size: 12480 }\\n})\\n```', metadata={'source': ''})],\n",
       " 'intermediate_steps': ['Size Limit is a performance budget tool for JavaScript that checks the size of JavaScript files and throws an'],\n",
       " 'output_text': '- Size Limit is a performance budget tool for JavaScript that checks the size of JavaScript files and throws an error if they exceed a specified limit.\\n- It is designed to help developers keep their JavaScript files small and optimize the performance of their websites or applications.\\n- The tool can be used in various ways, such as running it as a command-line tool, integrating it into a build process, or using it as a plugin in popular build tools like Webpack.\\n- Size Limit uses a combination of techniques to measure the size of JavaScript files, including analyzing the AST (Abstract Syntax Tree) of the code and calculating the size of the resulting bundle.\\n- It supports various file formats, including ES modules, CommonJS modules, and UMD (Universal Module Definition) modules.\\n- The tool allows developers to set a maximum size limit for their JavaScript files, either as an absolute value or as a percentage of the total bundle size.\\n- If a file exceeds the specified limit, Size Limit will throw an error and provide detailed information about the file, such as its size, the size limit, and the percentage of the total bundle size it represents.\\n- Size Limit also provides additional features to help developers optimize their JavaScript files, such as analyzing the dependencies of a file and suggesting ways to reduce its size.\\n- The tool can be configured to ignore certain files or directories, allowing developers to focus on optimizing specific parts of their codebase.\\n- Size Limit is highly customizable and can be configured using a configuration file or through command-line options.\\n- It supports various output formats, including plain text, JSON, and HTML, making it easy to integrate with existing build processes or tools.\\n- Size Limit is actively maintained and has a large community of users, which means that developers can rely on it for ongoing support and updates.\\n- The tool is open-source and available for free, allowing developers to use it without any licensing restrictions.\\n- Size Limit is compatible with modern JavaScript frameworks and libraries, such as React, Angular, and Vue.js, making it a versatile tool for optimizing JavaScript performance in a wide range of projects.'}"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "map_reduce_outputs[\"input_documents\"][0].page_content"
   ],
   "metadata": {
    "id": "QHyYiDQV0Snc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "map_reduce_outputs['output_text']"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "aqEw-e7F0Dtc",
    "outputId": "45418733-a92a-4647-eec6-8d7c71e0b811"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Size Limit is a performance budget tool for JavaScript that helps developers keep track of the size of their JavaScript files. It is designed to ensure that the size of the JavaScript code does not exceed a predefined limit, thus optimizing the performance of the application.\\n\\nThe tool works by analyzing the size of JavaScript files and comparing them to a specified limit. If the size exceeds the limit, Size Limit throws an error, alerting the developer to the issue. This allows developers to catch and address any potential performance bottlenecks early on in the development process.\\n\\nOne of the key features of Size Limit is its simplicity. It is easy to set up and use, making it accessible to developers of all skill levels. The tool can be integrated into existing projects with minimal effort, and it provides clear and concise error messages that make it easy to identify and fix any size-related issues.\\n\\nSize Limit also offers flexibility in terms of configuration. Developers can specify the maximum size limit for their JavaScript files, as well as define custom thresholds for different parts of the application. This allows for fine-grained control over the performance budget, ensuring that critical parts of the code are optimized while still allowing for some flexibility in other areas.\\n\\nIn addition to checking the size of JavaScript files, Size Limit also provides detailed reports and statistics. These reports include information such as the size of each file, the total size of the application, and the percentage of the budget used. This allows developers to gain insights into the size of their codebase and make informed decisions about optimization strategies.\\n\\nSize Limit is also highly customizable. Developers can configure the tool to ignore certain files or directories, exclude specific dependencies, or even set up custom rules for different parts of the application. This level of customization allows developers to tailor the tool to their specific needs and requirements.\\n\\nOverall, Size Limit is a powerful and user-friendly performance budget tool for JavaScript. It helps developers optimize the size of their JavaScript code and ensure that it stays within predefined limits. By catching and addressing size-related issues early on, developers can improve the performance and user experience of their applications. With its simplicity, flexibility, and detailed reporting capabilities, Size Limit is a valuable tool for any JavaScript developer looking to optimize their codebase.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 53
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meH2ELuz2H46"
   },
   "source": [
    "After summaries are generated, you can validate them by organize input documents and associated output in a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6FRSR7xRLew"
   },
   "outputs": [],
   "source": [
    "final_mp_data = []\n",
    "for doc, out in zip(\n",
    "    map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_mp_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA9cnh8YaNbF"
   },
   "outputs": [],
   "source": [
    "pdf_mp_summary = pd.DataFrame.from_dict(final_mp_data)\n",
    "pdf_mp_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the dataframe by filename and page_number\n",
    "pdf_mp_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_mp_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yA0eM1K3cvH2"
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "print(\"[Context]\")\n",
    "print(pdf_mp_summary[\"chunks\"].iloc[index])\n",
    "print(\"\\n\\n [Simple Summary]\")\n",
    "print(pdf_mp_summary[\"concise_summary\"].iloc[index])\n",
    "print(\"\\n\\n [Page number]\")\n",
    "print(pdf_mp_summary[\"page_number\"].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_mp_summary[\"file_name\"].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROrE1-HKpg7y"
   },
   "source": [
    "### Considerations\n",
    "\n",
    "With `MapReduce` method, the model is able to summarize a large paper by overcoming the context limit of `Stuffing` method with parallel processing.\n",
    "\n",
    "However, the `MapReduce` requires multiple calls to the model and potentially losing context between pages.\n",
    "\n",
    "To deal this challenge, you can try another method to summarize multiple pages at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxdB-5PqgCf-"
   },
   "source": [
    "## Method 3: Refine\n",
    "\n",
    "The Refine method is an alternative method to deal with large document summarization. It works by first running an initial prompt on a small chunk of data, generating some output. Then, for each subsequent document, the output from the previous document is passed in along with the new document, and the LLM is asked to refine the output based on the new document.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the load_summarize_chain method. What you need to do is setting `refine` as `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjj2UZilDF4Q"
   },
   "source": [
    "### Prompt design with `Refine` chain\n",
    "\n",
    "With LangChain, the `refine` chain requires two prompts.\n",
    "\n",
    "The question prompt to generate the output for subsequent task. The refine prompt to refine the output based on the generated content.\n",
    "\n",
    "In this example, the question prompt is:\n",
    "\n",
    "```\n",
    "Please provide a summary of the following text.\n",
    "TEXT: {text}\n",
    "SUMMARY:\n",
    "```\n",
    "\n",
    "and the refine prompt is:\n",
    "\n",
    "```\n",
    "Write a concise summary of the following text delimited by triple backquotes.\n",
    "Return your response in bullet points which covers the key points of the text.\n",
    "```{text}```\n",
    "BULLET POINT SUMMARY:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiZX45Z5VTwS"
   },
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "                  Please provide a summary of the following text.\n",
    "                  TEXT: {text}\n",
    "                  SUMMARY:\n",
    "                  \"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-USlaSPbM0rs"
   },
   "source": [
    "### Generate summaries using Refine method\n",
    "\n",
    "After you define prompts, you initiate a summarization chain using `refine` chain type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-Sv3HO1U3hi"
   },
   "outputs": [],
   "source": [
    "refine_chain = load_summarize_chain(\n",
    "    llm_gpt_turbo,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9EZCDK-MQJH"
   },
   "source": [
    "Then, you use the summatization chain to summarize document using Refine method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHwwab7vXNa1"
   },
   "outputs": [],
   "source": [
    "refine_outputs = refine_chain({\"input_documents\": [readme_doc]})"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "refine_outputs"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtuo_l7c0TI7",
    "outputId": "3b74df59-7699-40ae-9d38-126fa171f80b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='🦜️🔗 LangChain\\n⚡ Building applications with LLMs through composability ⚡\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLooking for the JS/TS version? Check out LangChain.js.\\nTo help you ship LangChain apps to production faster, check out LangSmith. \\nLangSmith is a unified developer platform for building, testing, and monitoring LLM applications. \\nFill out this form to get off the waitlist or speak with our sales team\\n🚨Breaking Changes for select chains (SQLDatabase) on 7/28/23\\nIn an effort to make langchain leaner and safer, we are moving select chains to langchain_experimental.\\nThis migration has already started, but we are remaining backwards compatible until 7/28.\\nOn that date, we will remove functionality from langchain.\\nRead more about the motivation and the progress here.\\nRead how to migrate your code here.\\nQuick Install\\npip install langchain\\nor\\npip install langsmith && conda install langchain -c conda-forge\\n🤔 What is this?\\nLarge language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. However, using these LLMs in isolation is often insufficient for creating a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\\nThis library aims to assist in the development of those types of applications. Common examples of these applications include:\\n❓ Question Answering over specific documents\\n\\nDocumentation\\nEnd-to-end Example: Question Answering over Notion Database\\n\\n💬 Chatbots\\n\\nDocumentation\\nEnd-to-end Example: Chat-LangChain\\n\\n🤖 Agents\\n\\nDocumentation\\nEnd-to-end Example: GPT+WolframAlpha\\n\\n📖 Documentation\\nPlease see here for full documentation on:\\n\\nGetting started (installation, setting up the environment, simple examples)\\nHow-To examples (demos, integrations, helper functions)\\nReference (full API docs)\\nResources (high-level explanation of core concepts)\\n\\n🚀 What can this help with?\\nThere are six main areas that LangChain is designed to help with.\\nThese are, in increasing order of complexity:\\n📃 LLMs and Prompts:\\nThis includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\\n🔗 Chains:\\nChains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\n📚 Data Augmented Generation:\\nData Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\\n🤖 Agents:\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\\n🧠 Memory:\\nMemory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\n🧐 Evaluation:\\n[BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is by using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\nFor more information on these concepts, please see our full documentation.\\n💁 Contributing\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\nFor detailed information on how to contribute, see here.', metadata={'source': 'https://en.wikipedia.org/wiki/Nuclear_power_in_space'})],\n",
       " 'intermediate_steps': ['The text is promoting LangChain, a library that aims to assist in the development of applications using large language models (LLMs). It highlights the benefits of using LLMs in combination with other sources of computation or knowledge. The library provides documentation and examples for various applications such as question answering, chatbots, and agents. It also offers support for prompt management, chains of LLM calls, data augmented generation, memory, and evaluation. The text encourages contributions to the open-source project.'],\n",
       " 'output_text': 'The text is promoting LangChain, a library that aims to assist in the development of applications using large language models (LLMs). It highlights the benefits of using LLMs in combination with other sources of computation or knowledge. The library provides documentation and examples for various applications such as question answering, chatbots, and agents. It also offers support for prompt management, chains of LLM calls, data augmented generation, memory, and evaluation. The text encourages contributions to the open-source project.'}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUqpki5EMYEr"
   },
   "source": [
    "Below you can see the resulting summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j5cUGStZ5WF"
   },
   "outputs": [],
   "source": [
    "final_refine_data = []\n",
    "for doc, out in zip(\n",
    "    refine_outputs[\"input_documents\"], refine_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_refine_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_7Mm9cEmGOV"
   },
   "outputs": [],
   "source": [
    "pdf_refine_summary = pd.DataFrame.from_dict(final_refine_data)\n",
    "pdf_refine_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the datafram by filename and page_number\n",
    "pdf_refine_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_refine_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvLVCs8Gbwbw"
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "print(\"[Context]\")\n",
    "print(pdf_refine_summary[\"chunks\"].iloc[index])\n",
    "print(\"\\n\\n [Simple Summary]\")\n",
    "print(pdf_refine_summary[\"concise_summary\"].iloc[index])\n",
    "print(\"\\n\\n [Page number]\")\n",
    "print(pdf_refine_summary[\"page_number\"].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_refine_summary[\"file_name\"].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dwgbRTrM5Cb"
   },
   "source": [
    "### Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H0Y5pPcXbgm"
   },
   "source": [
    "In short, the Refine method for text summarization with LLMs can pull in more relevant context and may be less lossy than Map Reduce. However, it requires many more calls to the LLM than Stuffing, and these calls are not independent, meaning they cannot be parallelized. Additionally, there is some potential dependency on the ordering of the documents. Latest documents they might become more relevant as this method suffers from recency bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAaWXncPMhv4"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "In this notebook you learn about different techniques to summarize long documents with LangChain and PaLM API. What you have seen in this notebook are only some of the possibilities you have. For example, there is another method called the Map-Rerank method which involves running an initial prompt on each chunk of data, which not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned.\n",
    "\n",
    "With that being said, it is important to highlight that depending on your needs you may consider to use pure Foundational model with a custom framework to build generative ai application.\n",
    "\n",
    "Here are some of the benefits of using a foundational model with a custom framework:\n",
    "\n",
    " - More flexibility to implement your application with different LLMs, prompting templates, document handling strategies and more.\n",
    "\n",
    " - More control to customize your generative applications based on your scenario.\n",
    "\n",
    " - Better performance to improve latency and scalability of your application.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
